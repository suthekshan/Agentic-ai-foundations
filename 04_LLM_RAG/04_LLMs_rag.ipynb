{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d657735",
   "metadata": {},
   "source": [
    "# From LLMs to the Breakdown of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be02827",
   "metadata": {},
   "source": [
    "## What is a Large Language Model (LLM)?\n",
    "\n",
    "A **Large Language Model (LLM)** is a type of artificial intelligence that understands and generates human language.\n",
    "\n",
    "It learns patterns from massive amounts of text (books, articles, code, etc.) and uses those patterns to predict the **next most likely word** in a sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## How an LLM Works \n",
    "\n",
    "1. You give the model some text (a **prompt**)\n",
    "2. The model processes the text using learned patterns\n",
    "3. It predicts the next word, then the next, and so on\n",
    "4. The result is a coherent **response**\n",
    "\n",
    "### Key Terms\n",
    "\n",
    "- **Prompt**: Input text given to the model\n",
    "- **Completion**: Text generated by the model\n",
    "- **Tokens**: Pieces of text (words or word fragments)\n",
    "- **Context Window**: How much text the model can remember at once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587042d7",
   "metadata": {},
   "source": [
    "<img src=\"images/llm_timeline.png\" alt=\"LLM timeline\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79eed9",
   "metadata": {},
   "source": [
    "Set API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94d6be8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda96c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16809365",
   "metadata": {},
   "source": [
    "## RAG System \n",
    "\n",
    "- PDF document ingestion\n",
    "- Text chunking\n",
    "- Hugging Face embeddings \n",
    "- ChromaDB vector store\n",
    "- LLM for Domain Specific grounded question answering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d40f07",
   "metadata": {},
   "source": [
    "<img src=\"images/RAG.jpg\" alt=\"LLM timeline\" width=\"800\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30862429",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "  langchain \\\n",
    "  langchain-core \\\n",
    "  langchain-community \\\n",
    "  langchain-chroma \\\n",
    "  langchain-text-splitters \\\n",
    "  langchain-groq \\\n",
    "  langchain-huggingface \\\n",
    "  sentence-transformers \\\n",
    "  pypdf \\\n",
    "  python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c45225",
   "metadata": {},
   "source": [
    "## üîê Environment Setup\n",
    "\n",
    "Create a `.env` file  with:\n",
    "\n",
    "        GROQ_API_KEY=your_groq_api_key_here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e91f23b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55647f6b",
   "metadata": {},
   "source": [
    "## Data ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073a6fa",
   "metadata": {},
   "source": [
    "<img src=\"images/data_ingestion.png\" alt=\"LLM timeline\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0f672",
   "metadata": {},
   "source": [
    "## üìÑ Load PDF Document\n",
    "\n",
    "We load the PDF using `PyPDFLoader`.  \n",
    "Each page becomes a `Document` object with metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "403cd225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_path = \"pdf1.pdf\"  # change if needed\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6718a",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Text Chunking\n",
    "\n",
    "The document is split into overlapping chunks to improve retrieval accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "203673ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)\n",
    "len(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61a4240",
   "metadata": {},
   "source": [
    "## üß† Embedding Model (Hugging Face)\n",
    "\n",
    "\n",
    "\n",
    "The embeddings are stored in ChromaDB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e49c7f",
   "metadata": {},
   "source": [
    "üí° Always use the same embedding model for a given Chroma directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "405fc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab2304",
   "metadata": {},
   "source": [
    "## üíæ ChromaDb Vector Store\n",
    "\n",
    "ChromaDB persists vectors on disk so embeddings are reused across runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "130c769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"pdf_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_pdf_db\"\n",
    ")\n",
    "\n",
    "# Add documents only once\n",
    "if vector_store._collection.count() == 0:\n",
    "    vector_store.add_documents(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f1f33",
   "metadata": {},
   "source": [
    "## üîç Retriever\n",
    "\n",
    "The retriever fetches the most relevant chunks for a given query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "afe5d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f44732",
   "metadata": {},
   "source": [
    "##   `k` in Retriever\n",
    "\n",
    "- `k` = number of most similar document chunks retrieved\n",
    "- Larger `k` ‚Üí more context, but more noise\n",
    "- Smaller `k` ‚Üí precise, but may miss info\n",
    "\n",
    "changing `k` affects the number of retrieved documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1bf76b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieved with k=1:\n",
      "Number of documents retrieved: 1\n",
      "\n",
      "--- Document 1 ---\n",
      "Content preview: History\n",
      "Topic Civilization & Culture\n",
      "Subtopic\n",
      "A History of India\n",
      "Professor Michael H. Fisher\n",
      "Oberlin College\n",
      "Course Guidebook...\n",
      " Retrieved with k=3:\n",
      "Number of documents retrieved: 3\n",
      "\n",
      "--- Document 1 ---\n",
      "Content preview: History\n",
      "Topic Civilization & Culture\n",
      "Subtopic\n",
      "A History of India\n",
      "Professor Michael H. Fisher\n",
      "Oberlin College\n",
      "Course Guidebook...\n",
      "\n",
      "--- Document 2 ---\n",
      "Content preview: leCTure 29‚ÄînaTionalisTs aMbedkar, bose, and JinnaH \n",
      "247\n",
      "SuggeSted Reading\n",
      "Bose, His Majesty‚Äôs Opponent.\n",
      "Jaffrelot, Dr. Ambedkar and Untouchability.\n",
      "Ja...\n",
      "\n",
      "--- Document 3 ---\n",
      "Content preview: v\n",
      "Table of ConTenTs \n",
      "LECTURE 24\n",
      "The British East India Company ............................... 194\n",
      "LECTURE 25\n",
      "The Issues and Events of 1857 .............\n",
      " Retrieved with k=5:\n",
      "Number of documents retrieved: 5\n",
      "\n",
      "--- Document 1 ---\n",
      "Content preview: History\n",
      "Topic Civilization & Culture\n",
      "Subtopic\n",
      "A History of India\n",
      "Professor Michael H. Fisher\n",
      "Oberlin College\n",
      "Course Guidebook...\n",
      "\n",
      "--- Document 2 ---\n",
      "Content preview: leCTure 29‚ÄînaTionalisTs aMbedkar, bose, and JinnaH \n",
      "247\n",
      "SuggeSted Reading\n",
      "Bose, His Majesty‚Äôs Opponent.\n",
      "Jaffrelot, Dr. Ambedkar and Untouchability.\n",
      "Ja...\n",
      "\n",
      "--- Document 3 ---\n",
      "Content preview: v\n",
      "Table of ConTenTs \n",
      "LECTURE 24\n",
      "The British East India Company ............................... 194\n",
      "LECTURE 25\n",
      "The Issues and Events of 1857 .............\n",
      "\n",
      "--- Document 4 ---\n",
      "Content preview: leCTure 24‚ÄîTHe briTisH easT india CoMpany \n",
      "203\n",
      "should be educated in their own languages. By ‚Äúeducation,‚Äù both \n",
      "sides really meant the education of a ...\n",
      "\n",
      "--- Document 5 ---\n",
      "Content preview: vi\n",
      "A History of indiA\n",
      "SUPPLEMENTAL MATERIAL\n",
      "Bibliography ................................................ 309\n",
      "Image Credits .............................\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "test_query = \"What is the main topic of this document?\"\n",
    "\n",
    "# Try different k values\n",
    "k_values = [1, 3, 5]\n",
    "\n",
    "for k in k_values:\n",
    "    # Create retriever with specific k\n",
    "    retriever_k = vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = retriever_k.invoke(test_query)\n",
    "    \n",
    "    print(f\" Retrieved with k={k}:\")\n",
    "    print(f\"Number of documents retrieved: {len(retrieved_docs)}\")\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\n--- Document {i} ---\")\n",
    "        print(f\"Content preview: {doc.page_content[:150]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a007788",
   "metadata": {},
   "source": [
    "## ‚ö° Groq LLM\n",
    "\n",
    "We use Groq‚Äôs hosted LLM.  \n",
    "Model runs remotely and is not stored locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "99e64179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0a0408",
   "metadata": {},
   "source": [
    "## üìù Prompt Template\n",
    "\n",
    "The LLM is instructed to answer strictly from retrieved context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "98505a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Use ONLY the context below to answer the question.\n",
    "If the answer is not present in the context, say:\n",
    "\"I do not know based on the provided document.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450ccb",
   "metadata": {},
   "source": [
    "## üîó RAG Chain\n",
    "\n",
    "The pipeline:\n",
    "Retriever ‚Üí Prompt ‚Üí LLM ‚Üí Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b412f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"Page {doc.metadata.get('page', 'N/A')}:\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a7fab8",
   "metadata": {},
   "source": [
    "## ‚ùì Ask Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "53aa1157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know based on the provided document.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"What does the document say about Ukraine?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb7c43e",
   "metadata": {},
   "source": [
    "## üö´ Out-of-Context Question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15d72dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know based on the provided document.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\n",
    "    \"What does the document say about Ukraine?\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
