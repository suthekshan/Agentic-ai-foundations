{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b36cb7",
   "metadata": {},
   "source": [
    "<Center> <Big> <b> Text-Generation using Transformers</b></Big></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5f1757",
   "metadata": {},
   "source": [
    "###  In this notebook, you will build a text generation pipeline using GPT-2. Instructions: Look for the ??? or ### YOUR CODE HERE markers and fill them in to make the code work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fbb5bc",
   "metadata": {},
   "source": [
    "### Task 1: Install the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2351d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b25ada",
   "metadata": {},
   "source": [
    "### Task 2: Load the pre-trained GPT-2 model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "model_name = \"???\" \n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode (turns off dropout)\n",
    "model.eval() \n",
    "print(\"Model Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75907c2",
   "metadata": {},
   "source": [
    "### Task 3: Convert text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea95f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Artificial intelligence will\"\n",
    "\n",
    "inputs = tokenizer(???, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Token IDs:\", inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7c157",
   "metadata": {},
   "source": [
    "### Task 4: Generate text!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac82eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_length=???,     \n",
    "    do_sample=True,     \n",
    "    temperature=0.7     \n",
    ")\n",
    "\n",
    "# Decode the output back into text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Text:\\n\" + 100*\"-\" + \"\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689dc522",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a24f55",
   "metadata": {},
   "source": [
    "##### SCENARIO A: The Boring Fact-Checker\n",
    "##### Goal: The model must complete the sentence logically and grammatically perfect. \n",
    "##### It should NOT go off-topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fact = \"The Earth revolves around the\"\n",
    "input_fact = tokenizer(prompt_fact, return_tensors=\"pt\")\n",
    "\n",
    "# TODO: Tune these values for maximum ACCURACY and REPETITION (Low randomness)\n",
    "outputs = model.generate(\n",
    "    **input_fact, \n",
    "    max_length=20,\n",
    "    do_sample=True,\n",
    "    temperature=???,   \n",
    "    top_k=???        \n",
    ")\n",
    "\n",
    "print(\"Scenario A Result:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a324e3",
   "metadata": {},
   "source": [
    "##### SCENARIO B: The Creative Storyteller\n",
    "##### Goal: The model should write something unexpected and diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73565592",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_story = \"Once upon a time in a galaxy far away,\"\n",
    "input_story = tokenizer(prompt_story, return_tensors=\"pt\")\n",
    "\n",
    "# TODO: Tune these values for maximum CREATIVITY (High randomness but coherent)\n",
    "outputs = model.generate(\n",
    "    **input_story, \n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=???,   \n",
    "    top_k=???          \n",
    ")\n",
    "\n",
    "print(\"Scenario B Result:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28de7db",
   "metadata": {},
   "source": [
    "Scenario C: The \"Broken\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a96259",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_debug = \"Machine learning is\"\n",
    "input_debug = tokenizer(prompt_debug, return_tensors=\"pt\")\n",
    "\n",
    "# FIXME: Change the values below to fix the output\n",
    "outputs = model.generate(\n",
    "    **input_debug, \n",
    "    max_length=1000,  \n",
    "    do_sample=True,\n",
    "    temperature=1.5, \n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "print(\"Fixed Result:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
