{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNWjEMpLNGY4"
      },
      "source": [
        "## 1. What is Text Generation?\n",
        "\n",
        "Text generation is the task of predicting the **next word/token** given previous text.\n",
        "\n",
        "\n",
        "<img src=\"https://tenor.com/en-GB/view/wallace-and-gromit-train-laying-tracks-rapid-u-turn-gif-7173982143403926265.gif\" align=\"right\" width=\"300\" style=\"margin-left: 20px;\">\n",
        "Examples:\n",
        "\n",
        "* Chatbots\n",
        "* Code generation\n",
        "* Story writing\n",
        "* Question answering\n",
        "* Autocomplete & smart replies\n",
        "\n",
        "\n",
        "> **Given some text → predict what comes next**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onPTs8wh-yxl"
      },
      "source": [
        "## 2. Traditional Approaches\n",
        "\n",
        "### N-grams\n",
        "\n",
        "* Look at last *n* words\n",
        "* Fails for long context\n",
        "\n",
        "### RNNs / LSTMs\n",
        "\n",
        "* Process text sequentially\n",
        "* Capture context better\n",
        "* ❌ Slow to train\n",
        "* ❌ Struggle with very long sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rc7NYPgNEby"
      },
      "source": [
        "**Transformers solve these problems**\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/R9zXHWAHyTjnq/giphy.gif\" width=\"300\" style=\"margin-left: 20px;\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxvYDaB5-0lz"
      },
      "source": [
        "## 3. Why Transformers?\n",
        "\n",
        "Transformers were introduced in the paper:\n",
        "\n",
        "> *\"Attention Is All You Need\"* (2017)\n",
        "\n",
        "\n",
        "* Process **all words in parallel**\n",
        "* Use **attention** to focus on important words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIgWej6i-9v3"
      },
      "source": [
        "## 4. High-Level Transformer Architecture\n",
        "\n",
        "A Transformer consists of:\n",
        "<img src=\"Images\\Tranformers.png\" align=\"right\" width=\"300\">\n",
        "\n",
        "1. **Embedding Layer**\n",
        "2. **Positional Encoding**\n",
        "3. **Self-Attention**\n",
        "4. **Feed Forward Network**\n",
        "5. **Layer Normalization & Residuals**\n",
        "6. **Output Softmax**\n",
        "\n",
        "For **text generation**, we use **Decoder-only Transformers**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwGKg1YK_Bh-"
      },
      "source": [
        "## 5. Tokenization\n",
        "\n",
        "\n",
        "<img src=\"https://tenor.com/en-GB/view/soori-pirichu-desingu-raja-gif-14335817.gif\" align=\"right\" width=\"300\">\n",
        "\n",
        "Neural networks dont understand text, they understand only numbers. \n",
        "So, to change it into vector represenetation we first divide them into individual tokens.\n",
        "\n",
        "### Tokenization:\n",
        "\n",
        "\"I love AI\" → `[\"I\", \"love\", \"AI\"]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw3WZXz1_GxV"
      },
      "source": [
        "## 6. Embeddings\n",
        "\n",
        "Each token is converted into a **vector**.\n",
        "\n",
        "Why?\n",
        "\n",
        "* Similar words → similar vectors\n",
        "* Captures semantic meaning\n",
        "\n",
        "<img src=\"Images\\embedding.png\" width=\"600\" >\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "* king - man + woman ≈ queen\n",
        "\n",
        "<img src=\"Images\\queen.png\" width=\"600\" >\n",
        "\n",
        "Other Examples:\n",
        "\n",
        "<img src=\"Images\\italy.png\" width=\"600\" >\n",
        "\n",
        "<img src=\"Images\\gender.png\" width=\"600\" >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss90mlyQ_MNF"
      },
      "source": [
        "## 7. Positional Encoding\n",
        "\n",
        "<img src=\"https://tenor.com/en-GB/view/my-spot-spot-ocd-sheldon-big-bang-theory-gif-7344971.gif\"  align=\"right\" width=\"300\">\n",
        "\n",
        "Each word is processed parallely ,so transformers **do not know word order** by default.\n",
        "\n",
        "\"Dog bites man\" ≠ \"Man bites dog\"\n",
        "\n",
        "So we add **position information** to embeddings.\n",
        "\n",
        "> This helps the model understand sequence order.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIbPh-PI_Rij"
      },
      "source": [
        "## 8. Attention — The Core Idea\n",
        "\n",
        "\n",
        "<img src=\"https://tenor.com/en-GB/view/broccoli-no-nope-dog-gif-11684736.gif\"  align=\"right\"  width=\"300\">\n",
        "\n",
        "When predicting a word, not all previous words are equally important.\n",
        "\n",
        "Example:\n",
        "\n",
        "> \"The capital of France is ___\"\n",
        "\n",
        "The model should **pay more attention** to:\n",
        "\n",
        "* capital\n",
        "* France\n",
        "\n",
        "and less to:\n",
        "\n",
        "* the\n",
        "* of"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvlp_2gb_WOb"
      },
      "source": [
        "## 9. Self-Attention\n",
        "<img src=\"https://tenor.com/en-GB/view/obama-obama-meme-obama-medal-medal-meme-gif-142382938442370469.gif\" align=\"right\" width=\"300\">\n",
        "\n",
        "In **self-attention**, each word:\n",
        "\n",
        "* Looks at **all other words**\n",
        "* Decides how much to focus on each\n",
        "\n",
        "This allows:\n",
        "\n",
        "* Long-range dependencies\n",
        "* Parallel computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubT44OT5_ZrW"
      },
      "source": [
        "## 10. Multi-Head Attention\n",
        "\n",
        "Instead of one attention mechanism:\n",
        "\n",
        "* Multiple attention heads run in parallel\n",
        "* Each head focuses on different relationships\n",
        "\n",
        "Example:\n",
        "\n",
        "* Head 1 → grammar\n",
        "* Head 2 → meaning\n",
        "* Head 3 → entities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S_VLnX2_eFF"
      },
      "source": [
        "## 11. Feed Forward Network\n",
        "\n",
        "<img src=\"Images\\neural-net.png\" align=\"right\" widht=\"250\">\n",
        "\n",
        "After attention:\n",
        "\n",
        "* Each token passes through a small neural network\n",
        "* Adds non-linearity\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> \"Refining\" the attended information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ECoP0Bm_vx9"
      },
      "source": [
        "## 12. Decoder-Only Transformers\n",
        "\n",
        "For text generation, we use:\n",
        "\n",
        "* **Masked self-attention**\n",
        "* Model can only see **previous tokens**, not future ones\n",
        "\n",
        "This prevents cheating during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_ZV8RJ__1pt"
      },
      "source": [
        "## 13. How Text Generation Works\n",
        "\n",
        "### Training:\n",
        "\n",
        "<img src=\"Images\\predict.png\" width=\"600\">\n",
        "\n",
        "Input:\n",
        "\n",
        "\"I love deep\" → Predict \"learning\"\n",
        "\n",
        "Loss:\n",
        "\n",
        "* Cross-entropy loss\n",
        "\n",
        "<img src=\"Images\\cross entropy.png\" width=\"300\">\n",
        "\n",
        "\n",
        "### Inference:\n",
        "\n",
        "1. Start with prompt\n",
        "2. Predict next token\n",
        "3. Append token\n",
        "4. Repeat\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxvfCzsD_6Do"
      },
      "source": [
        "## 14. Popular Transformer Models\n",
        "\n",
        "* GPT-2 / GPT-3 / GPT-4\n",
        "* BERT (not for generation)\n",
        "* T5\n",
        "* LLaMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmBMnU1R_7ZZ"
      },
      "source": [
        "# PRACTICAL SECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skxr7PXjNWcY"
      },
      "source": [
        "15. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pifqxrrwNMgH",
        "outputId": "a480ec37-c10c-4061-bf53-f0d7fdc10c92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzHgPpYANZTh"
      },
      "source": [
        "16. Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMbSZsJYNd_r"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwhMLqTQNv30"
      },
      "source": [
        "17. Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR-Irwn8NvC7",
        "outputId": "de284403-2e87-4449-a59b-e056c917287b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence will never be as simple as the humans may want it to be, but the possibilities are endless. It will change the way we interact with machines, and, like everything else in life, we will need to change it.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Artificial intelligence will\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "outputs = model.generate(\n",
        "**inputs,\n",
        "max_length=50,\n",
        "do_sample=True,\n",
        "temperature=0.7\n",
        ")\n",
        "\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-JBeGsdOIze"
      },
      "source": [
        "18. Important Generation Parameters\n",
        "**max_length**\n",
        "\n",
        "\n",
        "*   Maximum tokens generated\n",
        "\n",
        "\n",
        "**temperature**\n",
        "\n",
        "Controls randomness\n",
        "\n",
        "*   Low → deterministic\n",
        "*   High → creative\n",
        "\n",
        "\n",
        "**top_k / top_p**\n",
        "\n",
        "\n",
        "*   Limits vocabulary choices\n",
        "*   Prevents nonsense text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULBzxvO5OsQD"
      },
      "source": [
        "19. Controlled Generation Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYW4zw9NOyGO",
        "outputId": "5c0bffdd-74c1-4e94-b7bf-0355a385d53f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial intelligence will do some more of this.\n",
            "\n",
            "\"The AI system will be able to detect where people are when they're around and will be able to make more intelligent decisions,\" he said.\n",
            "\n",
            "\"It will also be able to help people get better at jobs.\n",
            "\n",
            "\"\n"
          ]
        }
      ],
      "source": [
        "outputs = model.generate(\n",
        "**inputs,\n",
        "max_length=60,\n",
        "do_sample=True,\n",
        "temperature=0.8,\n",
        "top_k=50,\n",
        "top_p=0.95\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibnX1Hj4NBN3"
      },
      "source": [
        "## 20. Why Transformers Work So Well\n",
        "\n",
        "* Parallel processing\n",
        "* Strong contextual understanding\n",
        "* Scales with data\n",
        "* Flexible architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO0pgeJGAQ9D"
      },
      "source": [
        "## 21. Limitations\n",
        "\n",
        "* Large models = high compute\n",
        "* Can hallucinate\n",
        "* Sensitive to prompts"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
