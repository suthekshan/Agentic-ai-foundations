{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c06db8",
   "metadata": {},
   "source": [
    "# Deep Learning Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ee754",
   "metadata": {},
   "source": [
    "## Neural Network Components\n",
    "\n",
    "- Input layer: takes raw features (e.g., image pixels as 2D/3D arrays).\n",
    "- Hidden layers: weighted sums + biases pass through activations to learn representations.\n",
    "- Activations: add nonlinearity (ReLU, sigmoid, tanh) so the model can fit complex patterns.\n",
    "- Output layer: produces task-specific predictions (e.g., class scores for cat/dog/bird).\n",
    "- Training: adjust weights/biases via backpropagation and gradient descent to reduce loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58ca2ff",
   "metadata": {},
   "source": [
    "![Alt text](images/nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00355a4",
   "metadata": {},
   "source": [
    "## Disadvantages of Feedforward Neural Networks\n",
    "\n",
    "- **No memory** – cannot remember past inputs\n",
    "- **No temporal dependency modeling** – treats each input independently\n",
    "- **Fixed-size input only** – all features must be given at once\n",
    "- **No sense of order or sequence**\n",
    "- **Poor performance on sequential data** (text, speech, time series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064a0c3",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a001b28c",
   "metadata": {},
   "source": [
    "## Why Do We Need Recurrent Neural Networks (RNNs)?\n",
    "\n",
    "- **Have memory** – store information from previous time steps\n",
    "- **Model temporal dependencies** in sequential data\n",
    "- **Process variable-length sequences**\n",
    "- **Order-aware** – understands sequence and context\n",
    "- **Better suited for time-based data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ab97d",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- Process sequences step by step, carrying a hidden state that summarizes prior tokens.\n",
    "- Parameter sharing across time makes them data-efficient for sequential patterns.\n",
    "- Limitations for text generation:\n",
    "  - Vanishing/exploding gradients hinder learning long-range dependencies.\n",
    "  - Hidden state is a bottleneck; context can fade over long spans.\n",
    "  - Strictly sequential computation limits parallelism, slowing training/inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d65ef",
   "metadata": {},
   "source": [
    "![Alt text](images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ec2c1",
   "metadata": {},
   "source": [
    "## RNN - Text Generation\n",
    "\n",
    "- Processes input **sequentially, one word at a time**\n",
    "- Each word is converted into a **word embedding** \\(c_t\\)\n",
    "- **Hidden state** \\(h^{(t)}\\) stores information from previous words\n",
    "- Hidden state is updated using current input and past memory\n",
    "- **Initial hidden state** \\(h^{(0)}\\) starts the sequence\n",
    "- **Softmax output layer** produces probability distribution over vocabulary\n",
    "- Predicts the **next word** based on previous context\n",
    "- Captures **temporal dependencies** in language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a775dfd1",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/rnntextgeneration.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5106393e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adf65856",
   "metadata": {},
   "source": [
    "## RNN Disadvantages\n",
    "\n",
    "- **Recurrent computation is slow** due to sequential processing\n",
    "- **Difficult to learn long-term dependencies**\n",
    "- **Vanishing and exploding gradient problems**\n",
    "- **Hard to access information from many time steps back**\n",
    "- **Training is computationally expensive**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a898cada",
   "metadata": {},
   "source": [
    "## Need for LSTM (Long Short-Term Memory)\n",
    "\n",
    "- Overcomes **vanishing gradient problem** in RNNs\n",
    "- Effectively learns **long-term dependencies**\n",
    "- Retains important information and **forgets irrelevant data**\n",
    "- Performs better on **long sequences**\n",
    "- More stable and reliable training than standard RNNs\n",
    "- Widely used in **language modeling, speech recognition, and time-series tasks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac20a82",
   "metadata": {},
   "source": [
    "## LSTM Architecture – Brief Overview\n",
    "\n",
    "- LSTM is a special type of RNN designed to model **long-term dependencies**\n",
    "- Consists of a **cell state (Cₜ)** that acts as long-term memory\n",
    "- Uses **three gates** to control information flow:\n",
    "  - **Forget Gate** – decides what information to remove from the cell state\n",
    "  - **Input Gate** – decides what new information to store\n",
    "  - **Output Gate** – decides what information to output\n",
    "- Gates use **sigmoid activation** to control data flow\n",
    "- **Tanh activation** is used to scale candidate values\n",
    "- Cell state flows through the network with minimal modification, reducing information loss\n",
    "- Produces a **hidden state (hₜ)** at each time step for output or next step processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6289c4",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/lstm.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b16e7",
   "metadata": {},
   "source": [
    "## LSTM - Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2a9d81",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/lstmtextgen.png\" alt=\"Alt text\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf9800",
   "metadata": {},
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras \n",
    "from keras.utils import np_utils\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871710bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras \n",
    "from keras.utils import np_utils\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca7cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e5751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_preparation(data):\n",
    "\n",
    "    # basic cleanup\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # create input sequences using list of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # pad sequences \n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # create predictors and label\n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = keras.utils.np_utils.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "    return predictors, label, max_sequence_len, total_words\n",
    "\n",
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences = True))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100,return_sequences = True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "    model.fit(predictors, label, epochs=100, verbose=1, callbacks=[earlystop])\n",
    "    print (model.summary())\n",
    "    return model \n",
    "\n",
    "def generate_text(seed_text, next_words, max_sequence_len,model):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list, verbose=0))\n",
    "    \n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "\n",
    "\n",
    "#data = open('data.txt').read()\n",
    "data = open('/content/drive/MyDrive/sample1.txt').read()\n",
    "predictors, label, max_sequence_len, total_words = dataset_preparation(data)\n",
    "model = create_model(predictors, label, max_sequence_len, total_words)\n",
    "\n",
    "print( generate_text(\"It can process \", 5, max_sequence_len,model))\n",
    "\n",
    "print(\"--Over--\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
